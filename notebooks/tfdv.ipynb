{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_data_validation as tfdv\n",
    "from google.protobuf import text_format\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from analyzers import DataType \n",
    "\n",
    "np.random.seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model on perturbed data.\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   35.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy', 'on train data: ', 0.8038262255878836)\n",
      "('accuracy', 'on test data: ', 0.8150510204081632)\n",
      "('accuracy', 'on target data: ', 0.8030592734225621)\n",
      "\n",
      "Training meta regressor on perturbed test data.\n",
      "\n",
      "Evaluating meta regressor on perturbed target data.\n",
      "MSE 0.00009, MAE 0.0075\n",
      "Writing plot to /home/reds/myrepo/unit-tests-ml-python/ssc/hilda/../figures/adult_income_balanced__missing_values_at_random__logistic_regression__accuracy.pdf\n",
      "reapply_perturbations\tadult_income_balanced\t0.8038262255878836\t0.8150510204081632\t0.8030592734225621\tlogistic_regression\taccuracy\tmissing_values_at_random\t9.237058083013448e-05\t0.007518214974007451\t/home/reds/myrepo/unit-tests-ml-python/ssc/hilda/../figures/adult_income_balanced__missing_values_at_random__logistic_regression__accuracy.pdf\n",
      "----------------------------------------------------------------------------------------------\n",
      "reapply_perturbations\tadult_income_balanced\t0.8038262255878836\t0.8150510204081632\t0.8030592734225621\tlogistic_regression\taccuracy\tmissing_values_at_random\t9.237058083013448e-05\t0.007518214974007451\t/home/reds/myrepo/unit-tests-ml-python/ssc/hilda/../figures/adult_income_balanced__missing_values_at_random__logistic_regression__accuracy.pdf\n"
     ]
    }
   ],
   "source": [
    "from ssc.hilda.datasets import *\n",
    "from ssc.hilda.perturbations import *\n",
    "from ssc.hilda.learners import *\n",
    "from ssc.hilda.experiments import *\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Pick a dataset\n",
    "# dataset = CardioDataset()\n",
    "dataset = BalancedAdultDataset()\n",
    "# dataset = AdultDataset()\n",
    "\n",
    "def gen_perturbations():\n",
    "    for num_columns_affected in range(1, 5):\n",
    "        for fraction_of_values_to_delete in [0.0, 0.05, 0.25, 0.5, 0.75, 0.99]:\n",
    "            for _ in range(100):\n",
    "                columns_affected = np.random.choice(dataset.categorical_columns, num_columns_affected)\n",
    "                yield MissingValues(fraction_of_values_to_delete, columns_affected, -1)\n",
    "\n",
    "# generate a bunch of perturbations for training\n",
    "perturbations_for_training = list(gen_perturbations())\n",
    "\n",
    "# generate a bunch of perturbations for evaluation\n",
    "perturbations_for_evaluation = list(gen_perturbations())\n",
    "\n",
    "# name the perturbations\n",
    "perturbations_name = \"missing_values_at_random\"\n",
    "\n",
    "# define the learner\n",
    "# learner = DNN('accuracy')\n",
    "# learner = LogisticRegression('roc_auc')\n",
    "learner = LogisticRegression('accuracy')\n",
    "\n",
    "# run an experiment\n",
    "log_line, model = reapply_perturbations(dataset, learner, perturbations_for_training,\n",
    "                                        perturbations_for_evaluation, perturbations_name)\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(log_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly short description</th>\n",
       "      <th>Anomaly long description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'education'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~39%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'marital_status'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~39%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'workclass'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~39%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Unnamed: 0'</th>\n",
       "      <td>New column</td>\n",
       "      <td>New column (column in data but not in schema)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'native_country'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: Holand-Netherlands (&lt;1%).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Anomaly short description  \\\n",
       "Feature name                                 \n",
       "'education'       Unexpected string values   \n",
       "'marital_status'  Unexpected string values   \n",
       "'workclass'       Unexpected string values   \n",
       "'Unnamed: 0'      New column                 \n",
       "'native_country'  Unexpected string values   \n",
       "\n",
       "                                                                     Anomaly long description  \n",
       "Feature name                                                                                   \n",
       "'education'       Examples contain values missing from the schema: -1 (~39%).                  \n",
       "'marital_status'  Examples contain values missing from the schema: -1 (~39%).                  \n",
       "'workclass'       Examples contain values missing from the schema: -1 (~39%).                  \n",
       "'Unnamed: 0'      New column (column in data but not in schema)                                \n",
       "'native_country'  Examples contain values missing from the schema: Holand-Netherlands (<1%).   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_test_split_csv(data_path, file_name, test_ratio=.2):\n",
    "    data = pd.read_csv(os.path.join(data_path, file_name))\n",
    "    train, test = train_test_split(data, test_size=test_ratio, random_state=1)\n",
    "    if not os.path.exists(os.path.join(data_path, 'tmp')):\n",
    "        os.makedirs(os.path.join(data_path, 'tmp'))\n",
    "    train.to_csv(os.path.join(data_path, 'tmp/train.csv'))\n",
    "    test.to_csv(os.path.join(data_path, 'tmp/test.csv'))\n",
    "\n",
    "data_path, file_name = \"/\".join(dataset.path.split('/')[:-1]), dataset.path.split('/')[-1]\n",
    "train_test_split_csv(data_path, file_name, test_ratio=.2)\n",
    "\n",
    "columns_affected = np.random.choice(dataset.categorical_columns, 3)\n",
    "(MissingValues(.4, columns_affected, -1)\n",
    "    .transform(pd.read_csv(os.path.join(data_path, 'tmp/test.csv')))\n",
    "    .to_csv(os.path.join(data_path, 'tmp/corrupted_test.csv')))\n",
    "\n",
    "train = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/train.csv'), delimiter=',')\n",
    "test = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/corrupted_test.csv'), delimiter=',')\n",
    "schema = tfdv.infer_schema(train)\n",
    "# print(schema)\n",
    "# tfdv.display_schema(schema)\n",
    "anomalies = tfdv.validate_statistics(statistics=test, schema=schema)\n",
    "# print(anomalies)\n",
    "tfdv.display_anomalies(anomalies)\n",
    "# print(text_format.MessageToString(anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRecordHelper:\n",
    "    class __TFRecordHelper:\n",
    "        def __init__(self):\n",
    "            self.foo = dict({\n",
    "                DataType.STRING: lambda x, y: x.bytes_list.value.extend([y]),\n",
    "                DataType.INTEGER: lambda x, y: x.int64_list.value.extend([y]),\n",
    "                DataType.FLOAT: lambda x, y: x.float_list.value.extend([y]),\n",
    "                DataType.OBJECT: lambda x, y: x.bytes_list.value.extend([y])\n",
    "            })\n",
    "            self.data_type = dict({\n",
    "                'int': DataType.INTEGER,\n",
    "                'int32': DataType.INTEGER,\n",
    "                'int64': DataType.INTEGER,\n",
    "                'float': DataType.FLOAT,\n",
    "                'float32': DataType.FLOAT,\n",
    "                'float64': DataType.FLOAT,\n",
    "                'byte': DataType.OBJECT,\n",
    "                # 'string': DataType.STRING,\n",
    "                'object': DataType.OBJECT\n",
    "            })\n",
    "\n",
    "        def run(self, example, feature_name, dtype, val):\n",
    "            if not isinstance(dtype, DataType):\n",
    "                dtype = self.data_type[str(dtype)]\n",
    "            return self.foo[dtype](example.features.feature[feature_name], val)\n",
    "\n",
    "    instance = None\n",
    "\n",
    "    def __init__(self):\n",
    "        if not TFRecordHelper.instance:\n",
    "            TFRecordHelper.instance = TFRecordHelper.__TFRecordHelper()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.instance, name)\n",
    "\n",
    "\n",
    "def convert_csv_to_tfrecord(data_path, file_name, dtypes=None):\n",
    "    filename = os.path.join(data_path, file_name.split('.')[0] + '.tfrecords')\n",
    "    data = pd.read_csv(os.path.join(data_path, file_name))\n",
    "    helper = TFRecordHelper()\n",
    "    columns = data.columns\n",
    "    if dtypes is None:\n",
    "        dtypes = data.dtypes\n",
    "    with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "        for i in range(data.shape[0]):\n",
    "            example = tf.train.Example()\n",
    "            for j in range(data.shape[1]):\n",
    "                helper.run(example, columns[j], dtypes[j], data.iloc[i, j])\n",
    "            writer.write(example.SerializeToString())\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrecord_filename = convert_csv_to_tfrecord(data_path, 'tmp/train.csv')\n",
    "test_tfrecord_filename = convert_csv_to_tfrecord(data_path, 'tmp/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "The following linter(s) triggered on your dataset:\n",
      "* NonNormalNumericFeatureDetector\n",
      "* TailedDistributionDetector\n",
      "\n",
      "\n",
      "================================================================================\n",
      "NonNormalNumericFeatureDetector\n",
      "================================================================================\n",
      "A feature flagged by this linter has a distribution that varies significantly\n",
      "from the other numeric features.\n",
      "Especially for linear models, poorly scaled features with high variance\n",
      "(e.g., all but one are in the range [-10, 10] but one is in [0, 100000])\n",
      "can wash out the effects of the other features.\n",
      "\n",
      "Quickfix: use the [standard score](https://en.wikipedia.org/wiki/Standard_score)\n",
      "of (at least) the flagged features.\n",
      "-----\n",
      "A 'typical' numeric feature in the dataset has mean 2.97e+04 and std dev 17589 but\n",
      "* fnlwgt had mean = 1.8999e+05, std_dev = 1.0595e+05\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TailedDistributionDetector\n",
      "================================================================================\n",
      "A feature flagged by this linter has an extremal value that significantly\n",
      "affects the mean. This may be because the value is an outlier but it may also\n",
      "be due to the extremal value being very common. In either case, however, it\n",
      "would be beneficial to check the histograms to ensure that they follow the\n",
      "expected distribution.\n",
      "\n",
      "Quickfix: check the histograms of the feature values.\n",
      "-----\n",
      "Flagged features and outlying extrema:\n",
      "* capital_gain: min value of 0\n",
      "* capital_loss: min value of 0\n",
      "================================================================================"
     ]
    }
   ],
   "source": [
    "dir_path = os.path.join(globals()['_dh'][0], '../third_party/data-linter')\n",
    "python = '/home/reds/install/miniconda3/envs/python2/bin/python'\n",
    "\n",
    "!{python} {dir_path}/demo/summarize_data.py --dataset_path {train_tfrecord_filename} \\\n",
    "  --stats_path /tmp/adult_summary.bin \\\n",
    "  --dataset_name adult\n",
    "\n",
    "!{python} {dir_path}/data_linter_main.py --dataset_path {test_tfrecord_filename} \\\n",
    "  --stats_path /tmp/adult_summary.bin \\\n",
    "  --results_path /tmp/datalinter/results/lint_results.bin\n",
    "\n",
    "!{python} {dir_path}/lint_explorer_main.py --results_path /tmp/datalinter/results/lint_results.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.model_selection._search.GridSearchCV'>\n",
      "<class 'sklearn.pipeline.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "from profilers import DataFrameProfiler, SklearnPipelineProfiler\n",
    "from test_suite import AutomatedTestSuite \n",
    "\n",
    "automated_suite = AutomatedTestSuite()\n",
    "data_profile = DataFrameProfiler().on(dataset.df)\n",
    "pipeline_profile = SklearnPipelineProfiler().on(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
