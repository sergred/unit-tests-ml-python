{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "from google.protobuf import text_format\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from analyzers import DataType \n",
    "\n",
    "np.random.seed = 1\n",
    "\n",
    "from ssc.hilda.datasets import *\n",
    "from ssc.hilda.perturbations import *\n",
    "from ssc.hilda.learners import *\n",
    "from ssc.hilda.experiments import *\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model on perturbed data.\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy', 'on train data: ', 0.8055001992825827)\n",
      "('accuracy', 'on test data: ', 0.8188775510204082)\n",
      "('accuracy', 'on target data: ', 0.7954110898661568)\n",
      "\n",
      "Training meta regressor on perturbed test data.\n",
      "\n",
      "Evaluating meta regressor on perturbed target data.\n",
      "MSE 0.00043, MAE 0.0182\n",
      "Writing plot to /home/reds/myrepo/unit-tests-ml-python/ssc/hilda/../figures/adult_income_balanced__missing_values_at_random__logistic_regression__accuracy.pdf\n",
      "/home/reds/myrepo/unit-tests-ml-python/ssc/hilda/../results/adult_income_balanced__missing_values_at_random__logistic_regression__accuracy.tsv\n",
      "reapply_perturbations\tadult_income_balanced\t0.8055001992825827\t0.8188775510204082\t0.7954110898661568\tlogistic_regression\taccuracy\tmissing_values_at_random\t0.00043434546984068523\t0.018177884558061585\t/home/reds/myrepo/unit-tests-ml-python/ssc/hilda/../figures/adult_income_balanced__missing_values_at_random__logistic_regression__accuracy.pdf\n"
     ]
    }
   ],
   "source": [
    "# Pick a dataset\n",
    "# dataset = CardioDataset()\n",
    "dataset = BalancedAdultDataset()\n",
    "# dataset = AdultDataset()\n",
    "\n",
    "def gen_perturbations():\n",
    "    for num_columns_affected in range(1, 5):\n",
    "        for fraction_of_values_to_delete in [0.0, 0.05, 0.25, 0.5, 0.75, 0.99]:\n",
    "            for _ in range(100):\n",
    "                columns_affected = np.random.choice(dataset.categorical_columns, num_columns_affected)\n",
    "                yield MissingValues(fraction_of_values_to_delete, columns_affected, -1)\n",
    "\n",
    "# generate a bunch of perturbations for training\n",
    "perturbations_for_training = gen_perturbations()\n",
    "\n",
    "# generate a bunch of perturbations for evaluation\n",
    "perturbations_for_evaluation = gen_perturbations()\n",
    "\n",
    "# name the perturbations\n",
    "perturbations_name = \"missing_values_at_random\"\n",
    "\n",
    "# define the learner\n",
    "# learner = DNN('accuracy')\n",
    "# learner = LogisticRegression('roc_auc')\n",
    "learner = LogisticRegression('accuracy')\n",
    "\n",
    "# run an experiment\n",
    "log_line, model, mse, mae = reapply_perturbations(dataset, learner, perturbations_for_training,\n",
    "                                                  perturbations_for_evaluation, perturbations_name)\n",
    "\n",
    "# print(\"----------------------------------------------------------------------------------------------\")\n",
    "# print(log_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFDV checks, data corruption - 40% of missing values in 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly short description</th>\n",
       "      <th>Anomaly long description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'occupation'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~39%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'marital_status'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~39%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'workclass'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~39%).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Anomaly short description  \\\n",
       "Feature name                                 \n",
       "'occupation'      Unexpected string values   \n",
       "'marital_status'  Unexpected string values   \n",
       "'workclass'       Unexpected string values   \n",
       "\n",
       "                                                      Anomaly long description  \n",
       "Feature name                                                                    \n",
       "'occupation'      Examples contain values missing from the schema: -1 (~39%).   \n",
       "'marital_status'  Examples contain values missing from the schema: -1 (~39%).   \n",
       "'workclass'       Examples contain values missing from the schema: -1 (~39%).   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path, file_name = \"/\".join(dataset.path.split('/')[:-1]), dataset.path.split('/')[-1]\n",
    "\n",
    "X_train, X_test, X_target = learner.split(dataset.df)\n",
    "\n",
    "# save train and test data and generate tfdv stats based on these csv files\n",
    "X_train.to_csv(os.path.join(data_path, 'tmp/X_train.csv'))\n",
    "X_test.to_csv(os.path.join(data_path, 'tmp/X_test.csv'))\n",
    "\n",
    "train_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_train.csv'), delimiter=',')\n",
    "test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_test.csv'), delimiter=',')\n",
    "\n",
    "# 40% missing values to 3 columns from the list of categorical features, saved to a csv file \n",
    "columns_affected = np.random.choice(dataset.categorical_columns, 3)\n",
    "corrupted_X_test = MissingValues(.4, columns_affected, -1).transform(X_test)\n",
    "corrupted_X_test.to_csv(os.path.join(data_path, 'tmp/corrupted_test_missing.csv'))\n",
    "\n",
    "# Generating stats basen on the corrupted data\n",
    "corrupted_test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/corrupted_test_missing.csv'), delimiter=',')\n",
    "\n",
    "# Inferring the schema and checking for schema violation\n",
    "schema = tfdv.infer_schema(train_stats)\n",
    "anomalies = tfdv.validate_statistics(statistics=corrupted_test_stats, schema=schema)\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-linter helping utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRecordHelper:\n",
    "    class __TFRecordHelper:\n",
    "        def __init__(self):\n",
    "            self.foo = dict({\n",
    "                DataType.STRING: lambda x, y: x.bytes_list.value.extend([y]),\n",
    "                DataType.INTEGER: lambda x, y: x.int64_list.value.extend([y]),\n",
    "                DataType.FLOAT: lambda x, y: x.float_list.value.extend([y]),\n",
    "                DataType.OBJECT: lambda x, y: x.bytes_list.value.extend([y])\n",
    "            })\n",
    "            self.data_type = dict({\n",
    "                'int': DataType.INTEGER,\n",
    "                'int32': DataType.INTEGER,\n",
    "                'int64': DataType.INTEGER,\n",
    "                'float': DataType.FLOAT,\n",
    "                'float32': DataType.FLOAT,\n",
    "                'float64': DataType.FLOAT,\n",
    "                'byte': DataType.OBJECT,\n",
    "                # 'string': DataType.STRING,\n",
    "                'object': DataType.OBJECT\n",
    "            })\n",
    "\n",
    "        def run(self, example, feature_name, dtype, val):\n",
    "            if not isinstance(dtype, DataType):\n",
    "                dtype = self.data_type[str(dtype)]\n",
    "            return self.foo[dtype](example.features.feature[feature_name], val)\n",
    "\n",
    "    instance = None\n",
    "\n",
    "    def __init__(self):\n",
    "        if not TFRecordHelper.instance:\n",
    "            TFRecordHelper.instance = TFRecordHelper.__TFRecordHelper()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.instance, name)\n",
    "\n",
    "\n",
    "def convert_csv_to_tfrecord(data_path, file_name, dtypes=None):\n",
    "    filename = os.path.join(data_path, file_name.split('.')[0] + '.tfrecords')\n",
    "    data = pd.read_csv(os.path.join(data_path, file_name))\n",
    "    helper = TFRecordHelper()\n",
    "    columns = data.columns\n",
    "    if dtypes is None:\n",
    "        dtypes = data.dtypes\n",
    "    with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "        for i in range(data.shape[0]):\n",
    "            example = tf.train.Example()\n",
    "            for j in range(data.shape[1]):\n",
    "                helper.run(example, columns[j], dtypes[j], data.iloc[i, j])\n",
    "            writer.write(example.SerializeToString())\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting csv files into tfrecords to work with data-linter\n",
    "train_tfrecord_filename = convert_csv_to_tfrecord(data_path, 'tmp/X_train.csv')\n",
    "test_tfrecord_filename = convert_csv_to_tfrecord(data_path, 'tmp/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "The following linter(s) triggered on your dataset:\n",
      "* NonNormalNumericFeatureDetector\n",
      "* TailedDistributionDetector\n",
      "\n",
      "\n",
      "================================================================================\n",
      "NonNormalNumericFeatureDetector\n",
      "================================================================================\n",
      "A feature flagged by this linter has a distribution that varies significantly\n",
      "from the other numeric features.\n",
      "Especially for linear models, poorly scaled features with high variance\n",
      "(e.g., all but one are in the range [-10, 10] but one is in [0, 100000])\n",
      "can wash out the effects of the other features.\n",
      "\n",
      "Quickfix: use the [standard score](https://en.wikipedia.org/wiki/Standard_score)\n",
      "of (at least) the flagged features.\n",
      "-----\n",
      "A 'typical' numeric feature in the dataset has mean 2.97e+04 and std dev 17801 but\n",
      "* fnlwgt had mean = 1.8911e+05, std_dev = 1.0417e+05\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TailedDistributionDetector\n",
      "================================================================================\n",
      "A feature flagged by this linter has an extremal value that significantly\n",
      "affects the mean. This may be because the value is an outlier but it may also\n",
      "be due to the extremal value being very common. In either case, however, it\n",
      "would be beneficial to check the histograms to ensure that they follow the\n",
      "expected distribution.\n",
      "\n",
      "Quickfix: check the histograms of the feature values.\n",
      "-----\n",
      "Flagged features and outlying extrema:\n",
      "* capital_gain: min value of 0\n",
      "* capital_loss: min value of 0\n",
      "================================================================================"
     ]
    }
   ],
   "source": [
    "# python2 required to work with data-linter\n",
    "python = [\"/home/reds/install/miniconda3/envs/python2/bin/python\"]\n",
    "\n",
    "dir_path = os.path.join(globals()['_dh'][0], '../third_party/data-linter')\n",
    "\n",
    "!{python[0]} {dir_path}/demo/summarize_data.py --dataset_path {train_tfrecord_filename} \\\n",
    "  --stats_path /tmp/adult_summary.bin \\\n",
    "  --dataset_name adult\n",
    "\n",
    "!{python[0]} {dir_path}/data_linter_main.py --dataset_path {test_tfrecord_filename} \\\n",
    "  --stats_path /tmp/adult_summary.bin \\\n",
    "  --results_path /tmp/datalinter/results/lint_results.bin\n",
    "\n",
    "!{python[0]} {dir_path}/lint_explorer_main.py --results_path /tmp/datalinter/results/lint_results.bin\n",
    "\n",
    "# linters are activated, detect anomalies in the data, and explain how these anomalies might be handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   57.4s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training meta regressor on perturbed test data.\n",
      "\n",
      "Evaluating meta regressor on perturbed target data.\n",
      "(array([0.04170154]), array([0.05135771]))\n",
      "WARNING! Performance drop: 0.0514 > 0.01, scores deviate by 0.0417\n"
     ]
    }
   ],
   "source": [
    "from jupyter_decorator import validate_on\n",
    "\n",
    "def gen_perturbations():\n",
    "    for num_columns_affected in range(1, 5):\n",
    "        for fraction_of_values_to_delete in [0.01, 0.05, 0.25, 0.4]:\n",
    "            for _ in range(100):\n",
    "                columns_affected = np.random.choice(dataset.categorical_columns, num_columns_affected)\n",
    "                yield MissingValues(fraction_of_values_to_delete, columns_affected, -1)\n",
    "\n",
    "y_test = dataset.labels_from(X_test)\n",
    "y_target = dataset.labels_from(X_target)\n",
    "\n",
    "# A decorator over the func that creates and trains the model, returns learner\n",
    "@validate_on(X_test, y_test, X_target, y_target)\n",
    "def learner_foo():\n",
    "    dataset = BalancedAdultDataset()\n",
    "    learner = LogisticRegression('accuracy')\n",
    "    # X_train = pd.read_csv(os.path.join(data_path, 'tmp/X_train.csv'))\n",
    "    model = learner.fit(dataset, X_train)\n",
    "    learner.model = model\n",
    "    learner.perturbations = gen_perturbations()\n",
    "    return learner\n",
    "\n",
    "\n",
    "learner = learner_foo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly short description</th>\n",
       "      <th>Anomaly long description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'occupation'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~70%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'education'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~70%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'marital_status'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~70%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'workclass'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~70%).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Anomaly short description  \\\n",
       "Feature name                                 \n",
       "'occupation'      Unexpected string values   \n",
       "'education'       Unexpected string values   \n",
       "'marital_status'  Unexpected string values   \n",
       "'workclass'       Unexpected string values   \n",
       "\n",
       "                                                      Anomaly long description  \n",
       "Feature name                                                                    \n",
       "'occupation'      Examples contain values missing from the schema: -1 (~70%).   \n",
       "'education'       Examples contain values missing from the schema: -1 (~70%).   \n",
       "'marital_status'  Examples contain values missing from the schema: -1 (~70%).   \n",
       "'workclass'       Examples contain values missing from the schema: -1 (~70%).   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns_affected = dataset.categorical_columns\n",
    "new_corrupted_X_test = MissingValues(.7, columns_affected, -1).transform(X_test)\n",
    "new_corrupted_X_test.to_csv(os.path.join(data_path, 'tmp/new_corrupted_test.csv'))\n",
    "\n",
    "train_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_train.csv'), delimiter=',')\n",
    "test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_test.csv'), delimiter=',')\n",
    "\n",
    "corrupted_test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/new_corrupted_test.csv'), delimiter=',')\n",
    "\n",
    "schema = tfdv.infer_schema(train_stats)\n",
    "anomalies = tfdv.validate_statistics(statistics=corrupted_test_stats, schema=schema)\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training meta regressor on perturbed test data.\n",
      "\n",
      "Evaluating meta regressor on perturbed target data.\n",
      "(array([0.20845324]), array([0.25672146]))\n",
      "WARNING! Performance drop: 0.2567 > 0.01, scores deviate by 0.2085\n"
     ]
    }
   ],
   "source": [
    "def gen_perturbations():\n",
    "    for num_columns_affected in range(1, 5):\n",
    "        for fraction_of_values_to_delete in [0.7, 0.8, 0.9]:\n",
    "            for _ in range(100):\n",
    "                columns_affected = dataset.categorical_columns\n",
    "                yield MissingValues(fraction_of_values_to_delete, columns_affected, -1)\n",
    "\n",
    "@validate_on(X_test, y_test, X_target, y_target)\n",
    "def learner_foo():\n",
    "    dataset = BalancedAdultDataset()\n",
    "    learner = LogisticRegression('accuracy')\n",
    "    # X_train = pd.read_csv(os.path.join(data_path, 'tmp/X_train.csv'))\n",
    "    model = learner.fit(dataset, X_train)\n",
    "    learner.model = model\n",
    "    learner.perturbations = gen_perturbations()\n",
    "    return learner\n",
    "\n",
    "\n",
    "learner = learner_foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4 style=\"color:green;\">No anomalies found.</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   54.9s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training meta regressor on perturbed test data.\n"
     ]
    }
   ],
   "source": [
    "columns_affected = dataset.numerical_columns\n",
    "new_corrupted_X_test = Outliers(.1, columns_affected).transform(X_test)\n",
    "new_corrupted_X_test.to_csv(os.path.join(data_path, 'tmp/corrupted_test_anomalies.csv'))\n",
    "\n",
    "train_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_train.csv'), delimiter=',')\n",
    "test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_test.csv'), delimiter=',')\n",
    "\n",
    "corrupted_test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/corrupted_test_anomalies.csv'), delimiter=',')\n",
    "\n",
    "schema = tfdv.infer_schema(train_stats)\n",
    "anomalies = tfdv.validate_statistics(statistics=corrupted_test_stats, schema=schema)\n",
    "tfdv.display_anomalies(anomalies)\n",
    "\n",
    "def gen_perturbations():\n",
    "    for num_columns_affected in range(1, 5):\n",
    "        for fraction_of_values_to_delete in [0.1, 0.05, 0.25]:\n",
    "            for _ in range(100):\n",
    "                columns_affected = dataset.numerical_columns\n",
    "                yield Outliers(fraction_of_values_to_delete, columns_affected)\n",
    "\n",
    "@validate_on(X_test, y_test, X_target, y_target)\n",
    "def learner_foo():\n",
    "    dataset = BalancedAdultDataset()\n",
    "    learner = LogisticRegression('accuracy')\n",
    "    # X_train = pd.read_csv(os.path.join(data_path, 'tmp/X_train.csv'))\n",
    "    model = learner.fit(dataset, X_train)\n",
    "    learner.model = model\n",
    "    learner.perturbations = gen_perturbations()\n",
    "    return learner\n",
    "\n",
    "\n",
    "learner = learner_foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrollingDataset()\n",
    "data_path, file_name = \"/\".join(dataset.path.split('/')[:-1]), dataset.path.split('/')[-1]\n",
    "X_train, X_test, X_target = learner.split(dataset.df)\n",
    "X_train.to_csv(os.path.join(data_path, 'tmp/X_train.csv'), sep='\\t')\n",
    "X_test.to_csv(os.path.join(data_path, 'tmp/X_test.csv'), sep='\\t')\n",
    "new_corrupted_X_test = Leetspeak(.01, 'content', 'label', 1).transform(X_test)\n",
    "new_corrupted_X_test.to_csv(os.path.join(data_path, 'tmp/corrupted_test_adversarial.csv'))\n",
    "\n",
    "train_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_train.csv'), delimiter='\\t')\n",
    "test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_test.csv'), delimiter='\\t')\n",
    "\n",
    "corrupted_test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/corrupted_test_adversarial.csv'), delimiter=',')\n",
    "\n",
    "schema = tfdv.infer_schema(train_stats)\n",
    "anomalies = tfdv.validate_statistics(statistics=corrupted_test_stats, schema=schema)\n",
    "tfdv.display_anomalies(anomalies)\n",
    "\n",
    "y_test = dataset.labels_from(X_test)\n",
    "y_target = dataset.labels_from(X_target)\n",
    "\n",
    "def gen_perturbations():\n",
    "    for fraction_of_values_to_delete in [0.1, 0.05, 0.25]:\n",
    "        for _ in range(500):\n",
    "            yield Leetspeak(fraction_of_values_to_delete, 'content', 'label', 1)\n",
    "\n",
    "@validate_on(X_test, y_test, X_target, y_target)\n",
    "def learner_foo():\n",
    "    dataset = TrollingDataset()\n",
    "    learner = LogisticRegression('accuracy')\n",
    "    # X_train = pd.read_csv(os.path.join(data_path, 'tmp/X_train.csv'))\n",
    "    model = learner.fit(dataset, X_train)\n",
    "    learner.model = model\n",
    "    learner.perturbations = gen_perturbations()\n",
    "    return learner\n",
    "\n",
    "\n",
    "learner = learner_foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
