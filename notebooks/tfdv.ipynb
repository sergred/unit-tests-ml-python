{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_data_validation as tfdv\n",
    "from google.protobuf import text_format\n",
    "# import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from analyzers import DataType\n",
    "from error_generation import ExplicitMissingValues\n",
    "\n",
    "np.random.seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRecordHelper:\n",
    "    class __TFRecordHelper:\n",
    "        def __init__(self):\n",
    "            self.foo = dict({\n",
    "                DataType.STRING: lambda x, y: x.bytes_list.value.extend([y]),\n",
    "                DataType.INTEGER: lambda x, y: x.int64_list.value.extend([y]),\n",
    "                DataType.FLOAT: lambda x, y: x.float_list.value.extend([y]),\n",
    "                DataType.OBJECT: lambda x, y: x.bytes_list.value.extend([y])\n",
    "            })\n",
    "            self.data_type = dict({\n",
    "                'int': DataType.INTEGER,\n",
    "                'int32': DataType.INTEGER,\n",
    "                'int64': DataType.INTEGER,\n",
    "                'float': DataType.FLOAT,\n",
    "                'float32': DataType.FLOAT,\n",
    "                'float64': DataType.FLOAT,\n",
    "                'byte': DataType.OBJECT,\n",
    "                # 'string': DataType.STRING,\n",
    "                'object': DataType.OBJECT\n",
    "            })\n",
    "\n",
    "        def run(self, example, feature_name, dtype, val):\n",
    "            if not isinstance(dtype, DataType):\n",
    "                dtype = self.data_type[str(dtype)]\n",
    "            return self.foo[dtype](example.features.feature[feature_name], val)\n",
    "\n",
    "    instance = None\n",
    "\n",
    "    def __init__(self):\n",
    "        if not TFRecordHelper.instance:\n",
    "            TFRecordHelper.instance = TFRecordHelper.__TFRecordHelper()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.instance, name)\n",
    "\n",
    "\n",
    "def convert_csv_to_tfrecord(data_path, file_name, dtypes=None):\n",
    "    filename = os.path.join(data_path, file_name.split('.')[0] + '.tfrecords')\n",
    "    data = pd.read_csv(os.path.join(data_path, file_name))\n",
    "    helper = TFRecordHelper()\n",
    "    columns = data.columns\n",
    "    if dtypes is None:\n",
    "        dtypes = data.dtypes\n",
    "    with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "        for i in range(data.shape[0]):\n",
    "            example = tf.train.Example()\n",
    "            for j in range(data.shape[1]):\n",
    "                helper.run(example, columns[j], dtypes[j], data.iloc[i, j])\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "\n",
    "def train_test_split_csv(data_path, file_name):\n",
    "    data = pd.read_csv(os.path.join(data_path, file_name))\n",
    "    train, test = train_test_split(data, test_size=0.33, random_state=1)\n",
    "    train.to_csv(os.path.join(data_path, 'train.csv'))\n",
    "    test.to_csv(os.path.join(data_path, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_validation(data_path):\n",
    "    train = tfdv.generate_statistics_from_csv(\n",
    "        os.path.join(data_path, 'train.csv'), delimiter=',')\n",
    "    test = tfdv.generate_statistics_from_csv(\n",
    "        os.path.join(data_path, 'train.csv'), delimiter=',')\n",
    "    schema = tfdv.infer_schema(train)\n",
    "    # print(schema)\n",
    "    # tfdv.display_schema(schema)\n",
    "    anomalies = tfdv.validate_statistics(statistics=test, schema=schema)\n",
    "    # print(anomalies)\n",
    "    tfdv.display_anomalies(anomalies)\n",
    "    # print(text_format.MessageToString(anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4 style=\"color:green;\">No anomalies found.</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = os.path.join('../resources/data/', 'wine-quality')\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "file_name = 'wine-quality-red.csv'\n",
    "convert_csv_to_tfrecord(data_path, file_name)\n",
    "# train_test_split_csv(data_path, file_name)\n",
    "ExplicitMissingValues().on(pd.read_csv(os.path.join(data_path, 'test_old.csv'))).to_csv(os.path.join(data_path, 'test.csv'))\n",
    "data_validation(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model on perturbed data.\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   25.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy', 'on train data: ', 0.8016739736946991)\n",
      "('accuracy', 'on test data: ', 0.8086734693877551)\n",
      "('accuracy', 'on target data: ', 0.798597833014659)\n",
      "\n",
      "Training meta regressor on perturbed test data.\n",
      "\n",
      "Evaluating meta regressor on perturbed target data.\n",
      "MSE 0.00017, MAE 0.0098\n",
      "Writing plot to /home/reds/myrepo/unit-tests-ml-python/ssc/hilda/../figures/adult_income_balanced__missing_values_at_random__logistic_regression__accuracy.pdf\n",
      "reapply_perturbations\tadult_income_balanced\t0.8016739736946991\t0.8086734693877551\t0.798597833014659\tlogistic_regression\taccuracy\tmissing_values_at_random\t0.00016677128574140687\t0.009775168259225333\t/home/reds/myrepo/unit-tests-ml-python/ssc/hilda/../figures/adult_income_balanced__missing_values_at_random__logistic_regression__accuracy.pdf\n",
      "----------------------------------------------------------------------------------------------\n",
      "reapply_perturbations\tadult_income_balanced\t0.8016739736946991\t0.8086734693877551\t0.798597833014659\tlogistic_regression\taccuracy\tmissing_values_at_random\t0.00016677128574140687\t0.009775168259225333\t/home/reds/myrepo/unit-tests-ml-python/ssc/hilda/../figures/adult_income_balanced__missing_values_at_random__logistic_regression__accuracy.pdf\n"
     ]
    }
   ],
   "source": [
    "from ssc.hilda.datasets import *\n",
    "from ssc.hilda.perturbations import *\n",
    "from ssc.hilda.learners import *\n",
    "from ssc.hilda.experiments import *\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Pick a dataset\n",
    "# dataset = CardioDataset()\n",
    "dataset = BalancedAdultDataset()\n",
    "# dataset = AdultDataset()\n",
    "\n",
    "def gen_perturbations():\n",
    "    for num_columns_affected in range(1, 5):\n",
    "        for fraction_of_values_to_delete in [0.0, 0.05, 0.25, 0.5, 0.75, 0.99]:\n",
    "            for _ in range(100):\n",
    "                columns_affected = np.random.choice(dataset.categorical_columns, num_columns_affected)\n",
    "                yield MissingValues(fraction_of_values_to_delete, columns_affected, -1)\n",
    "\n",
    "# generate a bunch of perturbations for training\n",
    "perturbations_for_training = list(gen_perturbations())\n",
    "\n",
    "# generate a bunch of perturbations for evaluation\n",
    "perturbations_for_evaluation = list(gen_perturbations())\n",
    "\n",
    "# name the perturbations\n",
    "perturbations_name = \"missing_values_at_random\"\n",
    "\n",
    "# define the learner\n",
    "# learner = DNN('accuracy')\n",
    "# learner = LogisticRegression('roc_auc')\n",
    "learner = LogisticRegression('accuracy')\n",
    "\n",
    "# run an experiment\n",
    "log_line = reapply_perturbations(dataset, learner, perturbations_for_training,\n",
    "                                 perturbations_for_evaluation, perturbations_name)\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(log_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
