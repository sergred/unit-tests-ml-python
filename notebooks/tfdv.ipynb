{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "from google.protobuf import text_format\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from analyzers import DataType \n",
    "\n",
    "np.random.seed = 1\n",
    "\n",
    "from ssc.hilda.datasets import *\n",
    "from ssc.hilda.perturbations import *\n",
    "from ssc.hilda.learners import *\n",
    "from ssc.hilda.experiments import *\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model on perturbed data.\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy', 'on train data: ', 0.8055001992825827)\n",
      "('accuracy', 'on test data: ', 0.7997448979591837)\n",
      "('accuracy', 'on target data: ', 0.8183556405353728)\n",
      "\n",
      "Training meta regressor on perturbed test data.\n",
      "\n",
      "Evaluating meta regressor on perturbed target data.\n",
      "MSE 0.00074, MAE 0.0255\n",
      "Writing plot to /home/reds/myrepo/unit-tests-ml-python/ssc/hilda/../figures/adult_income_balanced__missing_values_at_random__logistic_regression__accuracy.pdf\n",
      "/home/reds/myrepo/unit-tests-ml-python/ssc/hilda/../results/adult_income_balanced__missing_values_at_random__logistic_regression__accuracy.tsv\n",
      "reapply_perturbations\tadult_income_balanced\t0.8055001992825827\t0.7997448979591837\t0.8183556405353728\tlogistic_regression\taccuracy\tmissing_values_at_random\t0.0007410878587434928\t0.02547999029818268\t/home/reds/myrepo/unit-tests-ml-python/ssc/hilda/../figures/adult_income_balanced__missing_values_at_random__logistic_regression__accuracy.pdf\n"
     ]
    }
   ],
   "source": [
    "# Pick a dataset\n",
    "# dataset = CardioDataset()\n",
    "dataset = BalancedAdultDataset()\n",
    "# dataset = AdultDataset()\n",
    "\n",
    "def gen_perturbations():\n",
    "    for num_columns_affected in range(1, 5):\n",
    "        for fraction_of_values_to_delete in [0.0, 0.05, 0.25, 0.5, 0.75, 0.99]:\n",
    "            for _ in range(100):\n",
    "                columns_affected = np.random.choice(dataset.categorical_columns, num_columns_affected)\n",
    "                yield MissingValues(fraction_of_values_to_delete, columns_affected, -1)\n",
    "\n",
    "# generate a bunch of perturbations for training\n",
    "perturbations_for_training = gen_perturbations()\n",
    "\n",
    "# generate a bunch of perturbations for evaluation\n",
    "perturbations_for_evaluation = gen_perturbations()\n",
    "\n",
    "# name the perturbations\n",
    "perturbations_name = \"missing_values_at_random\"\n",
    "\n",
    "# define the learner\n",
    "# learner = DNN('accuracy')\n",
    "# learner = LogisticRegression('roc_auc')\n",
    "learner = LogisticRegression('accuracy')\n",
    "\n",
    "# run an experiment\n",
    "log_line, model, mse, mae = reapply_perturbations(dataset, learner, perturbations_for_training,\n",
    "                                                  perturbations_for_evaluation, perturbations_name)\n",
    "\n",
    "# print(\"----------------------------------------------------------------------------------------------\")\n",
    "# print(log_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly short description</th>\n",
       "      <th>Anomaly long description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'education'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~39%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'marital_status'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~39%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'workclass'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~39%).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Anomaly short description  \\\n",
       "Feature name                                 \n",
       "'education'       Unexpected string values   \n",
       "'marital_status'  Unexpected string values   \n",
       "'workclass'       Unexpected string values   \n",
       "\n",
       "                                                      Anomaly long description  \n",
       "Feature name                                                                    \n",
       "'education'       Examples contain values missing from the schema: -1 (~39%).   \n",
       "'marital_status'  Examples contain values missing from the schema: -1 (~39%).   \n",
       "'workclass'       Examples contain values missing from the schema: -1 (~39%).   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def train_test_split_csv(data_path, file_name, test_ratio=.2):\n",
    "#     data = pd.read_csv(os.path.join(data_path, file_name))\n",
    "#     train, test = train_test_split(data, test_size=test_ratio, random_state=1)\n",
    "#     if not os.path.exists(os.path.join(data_path, 'tmp')):\n",
    "#         os.makedirs(os.path.join(data_path, 'tmp'))\n",
    "#     train.to_csv(os.path.join(data_path, 'tmp/train.csv'))\n",
    "#     test.to_csv(os.path.join(data_path, 'tmp/test.csv'))\n",
    "\n",
    "data_path, file_name = \"/\".join(dataset.path.split('/')[:-1]), dataset.path.split('/')[-1]\n",
    "# train_test_split_csv(data_path, file_name, test_ratio=.2)\n",
    "\n",
    "X_train, X_test, X_target = learner.split(dataset.df)\n",
    "\n",
    "columns_affected = np.random.choice(dataset.categorical_columns, 3)\n",
    "corrupted_X_test = MissingValues(.4, columns_affected, -1).transform(X_test)\n",
    "corrupted_X_test.to_csv(os.path.join(data_path, 'tmp/corrupted_test_missing.csv'))\n",
    "\n",
    "X_train.to_csv(os.path.join(data_path, 'tmp/X_train.csv'))\n",
    "X_test.to_csv(os.path.join(data_path, 'tmp/X_test.csv'))\n",
    "\n",
    "train_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_train.csv'), delimiter=',')\n",
    "test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_test.csv'), delimiter=',')\n",
    "\n",
    "corrupted_test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/corrupted_test_missing.csv'), delimiter=',')\n",
    "\n",
    "schema = tfdv.infer_schema(train_stats)\n",
    "# print(schema)\n",
    "# tfdv.display_schema(schema)\n",
    "anomalies = tfdv.validate_statistics(statistics=corrupted_test_stats, schema=schema)\n",
    "# print(anomalies)\n",
    "tfdv.display_anomalies(anomalies)\n",
    "# print(text_format.MessageToString(anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRecordHelper:\n",
    "    class __TFRecordHelper:\n",
    "        def __init__(self):\n",
    "            self.foo = dict({\n",
    "                DataType.STRING: lambda x, y: x.bytes_list.value.extend([y]),\n",
    "                DataType.INTEGER: lambda x, y: x.int64_list.value.extend([y]),\n",
    "                DataType.FLOAT: lambda x, y: x.float_list.value.extend([y]),\n",
    "                DataType.OBJECT: lambda x, y: x.bytes_list.value.extend([y])\n",
    "            })\n",
    "            self.data_type = dict({\n",
    "                'int': DataType.INTEGER,\n",
    "                'int32': DataType.INTEGER,\n",
    "                'int64': DataType.INTEGER,\n",
    "                'float': DataType.FLOAT,\n",
    "                'float32': DataType.FLOAT,\n",
    "                'float64': DataType.FLOAT,\n",
    "                'byte': DataType.OBJECT,\n",
    "                # 'string': DataType.STRING,\n",
    "                'object': DataType.OBJECT\n",
    "            })\n",
    "\n",
    "        def run(self, example, feature_name, dtype, val):\n",
    "            if not isinstance(dtype, DataType):\n",
    "                dtype = self.data_type[str(dtype)]\n",
    "            return self.foo[dtype](example.features.feature[feature_name], val)\n",
    "\n",
    "    instance = None\n",
    "\n",
    "    def __init__(self):\n",
    "        if not TFRecordHelper.instance:\n",
    "            TFRecordHelper.instance = TFRecordHelper.__TFRecordHelper()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.instance, name)\n",
    "\n",
    "\n",
    "def convert_csv_to_tfrecord(data_path, file_name, dtypes=None):\n",
    "    filename = os.path.join(data_path, file_name.split('.')[0] + '.tfrecords')\n",
    "    data = pd.read_csv(os.path.join(data_path, file_name))\n",
    "    helper = TFRecordHelper()\n",
    "    columns = data.columns\n",
    "    if dtypes is None:\n",
    "        dtypes = data.dtypes\n",
    "    with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "        for i in range(data.shape[0]):\n",
    "            example = tf.train.Example()\n",
    "            for j in range(data.shape[1]):\n",
    "                helper.run(example, columns[j], dtypes[j], data.iloc[i, j])\n",
    "            writer.write(example.SerializeToString())\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrecord_filename = convert_csv_to_tfrecord(data_path, 'tmp/X_train.csv')\n",
    "test_tfrecord_filename = convert_csv_to_tfrecord(data_path, 'tmp/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "The following linter(s) triggered on your dataset:\n",
      "* NonNormalNumericFeatureDetector\n",
      "* NumberAsStringDetector\n",
      "* TailedDistributionDetector\n",
      "\n",
      "\n",
      "================================================================================\n",
      "NonNormalNumericFeatureDetector\n",
      "================================================================================\n",
      "A feature flagged by this linter has a distribution that varies significantly\n",
      "from the other numeric features.\n",
      "Especially for linear models, poorly scaled features with high variance\n",
      "(e.g., all but one are in the range [-10, 10] but one is in [0, 100000])\n",
      "can wash out the effects of the other features.\n",
      "\n",
      "Quickfix: use the [standard score](https://en.wikipedia.org/wiki/Standard_score)\n",
      "of (at least) the flagged features.\n",
      "-----\n",
      "A 'typical' numeric feature in the dataset has mean 2.96e+04 and std dev 17747 but\n",
      "* fnlwgt had mean = 1.8867e+05, std_dev = 1.0392e+05\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "NumberAsStringDetector\n",
      "================================================================================\n",
      "A feature flagged by this linter often takes values that look like numbers.\n",
      "For instance, it could contain simple floats, dollar values, or percents.\n",
      "\n",
      "Quickfix: unless the feature represents a categorical value, it would be better\n",
      "represented to the model as the number, itself.\n",
      "-----\n",
      "Flagged features (and sample values):\n",
      "* class: '>50K'\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TailedDistributionDetector\n",
      "================================================================================\n",
      "A feature flagged by this linter has an extremal value that significantly\n",
      "affects the mean. This may be because the value is an outlier but it may also\n",
      "be due to the extremal value being very common. In either case, however, it\n",
      "would be beneficial to check the histograms to ensure that they follow the\n",
      "expected distribution.\n",
      "\n",
      "Quickfix: check the histograms of the feature values.\n",
      "-----\n",
      "Flagged features and outlying extrema:\n",
      "* capital_gain: min value of 0\n",
      "* capital_loss: min value of 0\n",
      "================================================================================"
     ]
    }
   ],
   "source": [
    "python = [\"/home/reds/install/miniconda3/envs/python2/bin/python\"]\n",
    "\n",
    "dir_path = os.path.join(globals()['_dh'][0], '../third_party/data-linter')\n",
    "\n",
    "!{python[0]} {dir_path}/demo/summarize_data.py --dataset_path {train_tfrecord_filename} \\\n",
    "  --stats_path /tmp/adult_summary.bin \\\n",
    "  --dataset_name adult\n",
    "\n",
    "!{python[0]} {dir_path}/data_linter_main.py --dataset_path {test_tfrecord_filename} \\\n",
    "  --stats_path /tmp/adult_summary.bin \\\n",
    "  --results_path /tmp/datalinter/results/lint_results.bin\n",
    "\n",
    "!{python[0]} {dir_path}/lint_explorer_main.py --results_path /tmp/datalinter/results/lint_results.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_on(X_test, X_target):\n",
    "    def percentiles_of_probas(predictions):\n",
    "        probs_class_a = np.transpose(predictions)[0]\n",
    "        probs_class_b = np.transpose(predictions)[1]\n",
    "        features_a = np.percentile(probs_class_a, np.arange(0, 101, 5))\n",
    "        features_b = np.percentile(probs_class_b, np.arange(0, 101, 5))\n",
    "        return np.concatenate((features_a, features_b), axis=0)\n",
    "    \n",
    "    def validate(f):\n",
    "        def wrapper(*args):\n",
    "            y_test = dataset.labels_from(X_test)\n",
    "            y_target = dataset.labels_from(X_target)\n",
    "\n",
    "            learner = f(*args)\n",
    "            model = learner.model\n",
    "\n",
    "            # Training\n",
    "            print(\"\\nTraining meta regressor on perturbed test data.\")\n",
    "            perturbations_for_training = gen_perturbations()\n",
    "            meta_regressor = train_random_forest_regressor(X_test, y_test,\n",
    "                                                           perturbations_for_training,\n",
    "                                                           model, learner)\n",
    "\n",
    "            # Evaluation\n",
    "            print(\"\\nEvaluating meta regressor on perturbed target data.\")\n",
    "            predicted_scores, true_scores = [], []\n",
    "\n",
    "            for perturbation in learner.perturbations:\n",
    "                corrupted_target_data = perturbation.transform(X_target)\n",
    "\n",
    "                predictions = model.predict_proba(corrupted_target_data)\n",
    "                features = percentiles_of_probas(predictions)\n",
    "\n",
    "                score_on_corrupted_target_data = learner.score(y_target, model.predict(corrupted_target_data))\n",
    "                predicted_score_on_corrupted_target_data = meta_regressor.predict([features])\n",
    "\n",
    "                predicted_scores.append(predicted_score_on_corrupted_target_data)\n",
    "                true_scores.append(score_on_corrupted_target_data)\n",
    "\n",
    "            from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "            mse = mean_squared_error(true_scores, predicted_scores)\n",
    "            mae = mean_absolute_error(true_scores, predicted_scores)\n",
    "\n",
    "            print(\"MSE %.5f, MAE %.5f\" % (mse, mae))\n",
    "            \n",
    "            learner.meta_regressor = meta_regressor\n",
    "            \n",
    "            threshold = .01 # 1%\n",
    "                        \n",
    "            # Check whether mean of true_scores makes sense\n",
    "            print(np.mean(true_scores), mae / np.mean(true_scores))\n",
    "            # np.mean(np.abs(predicted_scores - true_scores) / true_scores) \n",
    "            if mae / np.mean(true_scores) > threshold:\n",
    "                print(\"WARNING! Performance drop\")\n",
    "            else:\n",
    "                print(\"Everything is fine\")\n",
    "            \n",
    "            return learner\n",
    "        return wrapper\n",
    "    return validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training meta regressor on perturbed test data.\n",
      "\n",
      "Evaluating meta regressor on perturbed target data.\n",
      "MSE 0.00017, MAE 0.01101\n",
      "(0.7875910877416613, 0.013983866923716283)\n",
      "WARNING! Performance drop\n"
     ]
    }
   ],
   "source": [
    "def gen_perturbations():\n",
    "    for num_columns_affected in range(1, 5):\n",
    "        for fraction_of_values_to_delete in [0.01, 0.05, 0.25, 0.4]:\n",
    "            for _ in range(100):\n",
    "                columns_affected = np.random.choice(dataset.categorical_columns, num_columns_affected)\n",
    "                yield MissingValues(fraction_of_values_to_delete, columns_affected, -1)\n",
    "\n",
    "@validate_on(X_test, X_target)\n",
    "def learner_foo():\n",
    "    dataset = BalancedAdultDataset()\n",
    "    learner = LogisticRegression('accuracy')\n",
    "    # X_train = pd.read_csv(os.path.join(data_path, 'tmp/X_train.csv'))\n",
    "    model = learner.fit(dataset, X_train)\n",
    "    learner.model = model\n",
    "    learner.perturbations = gen_perturbations()\n",
    "    return learner\n",
    "\n",
    "\n",
    "learner = learner_foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly short description</th>\n",
       "      <th>Anomaly long description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'occupation'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~70%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'education'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~70%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'marital_status'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~70%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'workclass'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: -1 (~70%).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Anomaly short description  \\\n",
       "Feature name                                 \n",
       "'occupation'      Unexpected string values   \n",
       "'education'       Unexpected string values   \n",
       "'marital_status'  Unexpected string values   \n",
       "'workclass'       Unexpected string values   \n",
       "\n",
       "                                                      Anomaly long description  \n",
       "Feature name                                                                    \n",
       "'occupation'      Examples contain values missing from the schema: -1 (~70%).   \n",
       "'education'       Examples contain values missing from the schema: -1 (~70%).   \n",
       "'marital_status'  Examples contain values missing from the schema: -1 (~70%).   \n",
       "'workclass'       Examples contain values missing from the schema: -1 (~70%).   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns_affected = dataset.categorical_columns\n",
    "new_corrupted_X_test = MissingValues(.7, columns_affected, -1).transform(X_test)\n",
    "new_corrupted_X_test.to_csv(os.path.join(data_path, 'tmp/new_corrupted_test.csv'))\n",
    "\n",
    "train_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_train.csv'), delimiter=',')\n",
    "test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_test.csv'), delimiter=',')\n",
    "\n",
    "corrupted_test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/new_corrupted_test.csv'), delimiter=',')\n",
    "\n",
    "schema = tfdv.infer_schema(train_stats)\n",
    "# print(schema)\n",
    "# tfdv.display_schema(schema)\n",
    "anomalies = tfdv.validate_statistics(statistics=corrupted_test_stats, schema=schema)\n",
    "# print(anomalies)\n",
    "tfdv.display_anomalies(anomalies)\n",
    "# print(text_format.MessageToString(anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training meta regressor on perturbed test data.\n",
      "\n",
      "Evaluating meta regressor on perturbed target data.\n",
      "MSE 0.00004, MAE 0.00466\n",
      "(0.615640535372849, 0.007572033125112053)\n",
      "Everything is fine\n"
     ]
    }
   ],
   "source": [
    "def gen_perturbations():\n",
    "    for num_columns_affected in range(1, 5):\n",
    "        for fraction_of_values_to_delete in [0.7, 0.8, 0.9]:\n",
    "            for _ in range(100):\n",
    "                columns_affected = dataset.categorical_columns\n",
    "                yield MissingValues(fraction_of_values_to_delete, columns_affected, -1)\n",
    "\n",
    "@validate_on(X_test, X_target)\n",
    "def learner_foo():\n",
    "    dataset = BalancedAdultDataset()\n",
    "    learner = LogisticRegression('accuracy')\n",
    "    # X_train = pd.read_csv(os.path.join(data_path, 'tmp/X_train.csv'))\n",
    "    model = learner.fit(dataset, X_train)\n",
    "    learner.model = model\n",
    "    learner.perturbations = gen_perturbations()\n",
    "    return learner\n",
    "\n",
    "\n",
    "learner = learner_foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4 style=\"color:green;\">No anomalies found.</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   58.1s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training meta regressor on perturbed test data.\n",
      "\n",
      "Evaluating meta regressor on perturbed target data.\n",
      "MSE 0.00016, MAE 0.01098\n",
      "(0.7948868706182282, 0.013812506496812631)\n",
      "WARNING! Performance drop\n"
     ]
    }
   ],
   "source": [
    "columns_affected = dataset.numerical_columns\n",
    "new_corrupted_X_test = Outliers(.1, columns_affected).transform(X_test)\n",
    "new_corrupted_X_test.to_csv(os.path.join(data_path, 'tmp/corrupted_test_anomalies.csv'))\n",
    "\n",
    "train_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_train.csv'), delimiter=',')\n",
    "test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_test.csv'), delimiter=',')\n",
    "\n",
    "corrupted_test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/corrupted_test_anomalies.csv'), delimiter=',')\n",
    "\n",
    "schema = tfdv.infer_schema(train_stats)\n",
    "# print(schema)\n",
    "# tfdv.display_schema(schema)\n",
    "anomalies = tfdv.validate_statistics(statistics=corrupted_test_stats, schema=schema)\n",
    "# print(anomalies)\n",
    "tfdv.display_anomalies(anomalies)\n",
    "# print(text_format.MessageToString(anomalies))\n",
    "\n",
    "def gen_perturbations():\n",
    "    for num_columns_affected in range(1, 5):\n",
    "        for fraction_of_values_to_delete in [0.1, 0.05, 0.25]:\n",
    "            for _ in range(100):\n",
    "                columns_affected = dataset.numerical_columns\n",
    "                yield Outliers(fraction_of_values_to_delete, columns_affected)\n",
    "\n",
    "@validate_on(X_test, X_target)\n",
    "def learner_foo():\n",
    "    dataset = BalancedAdultDataset()\n",
    "    learner = LogisticRegression('accuracy')\n",
    "    # X_train = pd.read_csv(os.path.join(data_path, 'tmp/X_train.csv'))\n",
    "    model = learner.fit(dataset, X_train)\n",
    "    learner.model = model\n",
    "    learner.perturbations = gen_perturbations()\n",
    "    return learner\n",
    "\n",
    "\n",
    "learner = learner_foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/reds/myrepo/unit-tests-ml-python/ssc/hilda/../../resources/data/trolls/data.tsv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No line was found. [while running 'DecodeData/ParseCSVRecords']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5fea272653b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnew_corrupted_X_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tmp/corrupted_test_adversarial.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_statistics_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tmp/X_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtest_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_statistics_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tmp/X_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/tensorflow_data_validation/utils/stats_gen_lib.pyc\u001b[0m in \u001b[0;36mgenerate_statistics_from_csv\u001b[0;34m(data_location, column_names, delimiter, output_path, stats_options, pipeline_options)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mshard_name_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             coder=beam.coders.ProtoCoder(\n\u001b[0;32m--> 158\u001b[0;31m                 statistics_pb2.DatasetFeatureStatisticsList)))\n\u001b[0m\u001b[1;32m    159\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    403\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_fake_coders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m           self._options).run(False)\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTypeOptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime_type_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.pyc\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline)\u001b[0m\n\u001b[1;32m    137\u001b[0m       \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBundleBasedDirectRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/portability/fn_api_runner.pyc\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline)\u001b[0m\n\u001b[1;32m    247\u001b[0m     self._profiler_factory = profiler.Profile.factory_from_options(\n\u001b[1;32m    248\u001b[0m         pipeline._options.view_as(pipeline_options.ProfilingOptions))\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_via_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_via_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/portability/fn_api_runner.pyc\u001b[0m in \u001b[0;36mrun_via_runner_api\u001b[0;34m(self, pipeline_proto)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_via_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_stages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/portability/fn_api_runner.pyc\u001b[0m in \u001b[0;36mrun_stages\u001b[0;34m(self, pipeline_components, stages, safe_coders)\u001b[0m\n\u001b[1;32m   1059\u001b[0m           stage_results = self.run_stage(\n\u001b[1;32m   1060\u001b[0m               \u001b[0mcontroller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m               pcoll_buffers, safe_coders)\n\u001b[0m\u001b[1;32m   1062\u001b[0m           \u001b[0mmetrics_by_stage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_bundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           monitoring_infos_by_stage[stage.name] = (\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/portability/fn_api_runner.pyc\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self, controller, pipeline_components, stage, pcoll_buffers, safe_coders)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     result = BundleManager(\n\u001b[1;32m   1189\u001b[0m         \u001b[0mcontroller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_bundle_descriptor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m         self._progress_frequency).process_bundle(data_input, data_output)\n\u001b[0m\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/portability/fn_api_runner.pyc\u001b[0m in \u001b[0;36mprocess_bundle\u001b[0;34m(self, inputs, expected_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m         process_bundle=beam_fn_api_pb2.ProcessBundleRequest(\n\u001b[1;32m   1499\u001b[0m             process_bundle_descriptor_reference=self._bundle_descriptor.id))\n\u001b[0;32m-> 1500\u001b[0;31m     \u001b[0mresult_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     with ProgressRequester(\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/portability/fn_api_runner.pyc\u001b[0m in \u001b[0;36mpush\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'control_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uid_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CONTROL REQUEST %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_instruction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CONTROL RESPONSE %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mControlFuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/worker/sdk_worker.pyc\u001b[0m in \u001b[0;36mdo_instruction\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;31m# E.g. if register is set, this will call self.register(request.register))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m       return getattr(self, request_type)(getattr(request, request_type),\n\u001b[0;32m--> 221\u001b[0;31m                                          request.instruction_id)\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/worker/sdk_worker.pyc\u001b[0m in \u001b[0;36mprocess_bundle\u001b[0;34m(self, request, instruction_id)\u001b[0m\n\u001b[1;32m    235\u001b[0m         request.process_bundle_descriptor_reference) as bundle_processor:\n\u001b[1;32m    236\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mbundle_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_bundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m       return beam_fn_api_pb2.InstructionResponse(\n\u001b[1;32m    239\u001b[0m           \u001b[0minstruction_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstruction_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/worker/bundle_processor.pyc\u001b[0m in \u001b[0;36mprocess_bundle\u001b[0;34m(self, instruction_id)\u001b[0m\n\u001b[1;32m    434\u001b[0m           input_op_by_target[\n\u001b[1;32m    435\u001b[0m               \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitive_transform_reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m           ].process_encoded(data.data)\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;31m# Finish all operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/worker/bundle_processor.pyc\u001b[0m in \u001b[0;36mprocess_encoded\u001b[0;34m(self, encoded_windowed_values)\u001b[0m\n\u001b[1;32m    123\u001b[0m       decoded_value = self.windowed_coder_impl.decode_from_stream(\n\u001b[1;32m    124\u001b[0m           input_stream, True)\n\u001b[0;32m--> 125\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/worker/operations.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.Operation.output\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/worker/operations.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.Operation.output\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/worker/operations.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.ConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/worker/operations.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.ImpulseReadOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/worker/operations.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.ImpulseReadOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/worker/operations.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.Operation.output\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/worker/operations.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.ConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/worker/operations.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/worker/operations.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/common.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.receive\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/common.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/common.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner._reraise_augmented\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/common.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/runners/common.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.SimpleInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/apache_beam/transforms/core.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m     \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m   \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Map(%s)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mptransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_from_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/tensorflow_data_validation/coders/csv_decoder.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, csv_string)\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m\"\"\"Parse a CSV record into a list of strings.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/tensorflow_data_validation/coders/csv_decoder.pyc\u001b[0m in \u001b[0;36mread_record\u001b[0;34m(self, csv_string)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_utf8_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/reds/install/miniconda3/envs/python2/lib/python2.7/site-packages/tensorflow_data_validation/coders/csv_decoder.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mnum_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_lines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No line was found.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mnum_lines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unexpected number of lines %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No line was found. [while running 'DecodeData/ParseCSVRecords']"
     ]
    }
   ],
   "source": [
    "dataset = TrollingDataset()\n",
    "data_path, file_name = \"/\".join(dataset.path.split('/')[:-1]), dataset.path.split('/')[-1]\n",
    "X_train, X_test, X_target = learner.split(dataset.df)\n",
    "X_train.to_csv(os.path.join(data_path, 'tmp/X_train.csv'), sep='\\t')\n",
    "X_test.to_csv(os.path.join(data_path, 'tmp/X_test.csv'), sep='\\t')\n",
    "new_corrupted_X_test = Leetspeak(.01, 'content', 'label', 1).transform(X_test)\n",
    "new_corrupted_X_test.to_csv(os.path.join(data_path, 'tmp/corrupted_test_adversarial.csv'))\n",
    "\n",
    "train_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_train.csv'), delimiter='\\t')\n",
    "test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/X_test.csv'), delimiter='\\t')\n",
    "\n",
    "corrupted_test_stats = tfdv.generate_statistics_from_csv(os.path.join(data_path, 'tmp/corrupted_test_adversarial.csv'), delimiter=',')\n",
    "\n",
    "schema = tfdv.infer_schema(train_stats)\n",
    "# print(schema)\n",
    "# tfdv.display_schema(schema)\n",
    "anomalies = tfdv.validate_statistics(statistics=corrupted_test_stats, schema=schema)\n",
    "# print(anomalies)\n",
    "tfdv.display_anomalies(anomalies)\n",
    "# print(text_format.MessageToString(anomalies))\n",
    "\n",
    "def gen_perturbations():\n",
    "    for fraction_of_values_to_delete in [0.1, 0.05, 0.25]:\n",
    "        for _ in range(500):\n",
    "            yield Leetspeak(fraction_of_values_to_delete, 'content', 'label', 1)\n",
    "\n",
    "@validate_on(X_test, X_target)\n",
    "def learner_foo():\n",
    "    dataset = BalancedAdultDataset()\n",
    "    learner = LogisticRegression('accuracy')\n",
    "    # X_train = pd.read_csv(os.path.join(data_path, 'tmp/X_train.csv'))\n",
    "    model = learner.fit(dataset, X_train)\n",
    "    learner.model = model\n",
    "    learner.perturbations = gen_perturbations()\n",
    "    return learner\n",
    "\n",
    "\n",
    "learner = learner_foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
